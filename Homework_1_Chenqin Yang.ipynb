{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSGA-1011 Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from random import *\n",
    "from nose.tools import assert_equal\n",
    "from tqdm import tqdm_notebook\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from nltk import ngrams\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "train_pos_path = './aclImdb/train/pos/'\n",
    "train_neg_path = './aclImdb/train/neg/'\n",
    "test_pos_path = './aclImdb/test/pos/'\n",
    "test_neg_path = './aclImdb/test/neg/'\n",
    "\n",
    "# merge the file contents into the list\n",
    "# assign 1 to positive reviews and 0 to negatie reviews\n",
    "def file2list(path,y):\n",
    "    file2list = []\n",
    "    for file in os.listdir(path):\n",
    "        with open(path+file,'r') as filecontent:\n",
    "            file2list.append((filecontent.read(),y))    \n",
    "    return file2list\n",
    "\n",
    "train_pos = file2list(train_pos_path,1)\n",
    "train_neg = file2list(train_neg_path,0)\n",
    "test_pos = file2list(test_pos_path,1)\n",
    "test_neg = file2list(test_neg_path,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of train_pos is 12500\n",
      "The size of train_neg is 12500\n",
      "The size of test_pos is 12500\n",
      "The size of test_neg is 12500\n"
     ]
    }
   ],
   "source": [
    "# Check the size of each loaded list\n",
    "print('The size of train_pos is {}'.format(len(train_pos)))\n",
    "print('The size of train_neg is {}'.format(len(train_neg)))\n",
    "print('The size of test_pos is {}'.format(len(test_pos)))\n",
    "print('The size of test_neg is {}'.format(len(test_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_target(dataset_pos,dataset_neg):\n",
    "    dataset = dataset_pos + dataset_neg\n",
    "    dataset_data = []\n",
    "    dataset_target = []\n",
    "    for review in dataset:\n",
    "        dataset_data.append(review[0])\n",
    "        dataset_target.append(review[1])\n",
    "    return dataset_data, dataset_target\n",
    "\n",
    "train_data1, train_target = get_data_target(train_pos,train_neg)\n",
    "test_data, test_target = get_data_target(test_pos,test_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data and targets are created correctly \n",
    "# no errors should rise\n",
    "a = randint(0,len(train_pos))\n",
    "b = 12500 + a\n",
    "assert_equal(train_pos[a][0],train_data1[a])\n",
    "assert_equal(train_neg[a][0],train_data1[b])\n",
    "assert_equal(test_pos[a][0],test_data[a])\n",
    "assert_equal(test_neg[a][0],test_data[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of train_data is 25000\n",
      "The size of train_target is 25000\n",
      "The size of test_data is 25000\n",
      "The size of test_target is 25000\n"
     ]
    }
   ],
   "source": [
    "print('The size of train_data is {}'.format(len(train_data1)))\n",
    "print('The size of train_target is {}'.format(len(train_target)))\n",
    "print('The size of test_data is {}'.format(len(test_data)))\n",
    "print('The size of test_target is {}'.format(len(test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Validation dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# Split train data into actual train and validation sets\n",
    "train_split = 20000\n",
    "train_data = train_data1[:train_split]\n",
    "train_targets = train_target[:train_split]\n",
    "\n",
    "val_data = train_data1[train_split:]\n",
    "val_targets = train_target[train_split:]\n",
    "\n",
    "test_data = test_data\n",
    "test_targets = test_target\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data)))\n",
    "print (\"Validation dataset size is {}\".format(len(val_data)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove HTML tag\n",
    "train_data = [s.replace('<br />', ' ') for s in train_data]\n",
    "train_data = [s.replace('  ', ' ') for s in train_data]\n",
    "\n",
    "val_data = [s.replace('<br />', ' ') for s in val_data]\n",
    "val_data = [s.replace('  ', ' ') for s in val_data]\n",
    "\n",
    "test_data = [s.replace('<br />', ' ') for s in test_data]\n",
    "test_data = [s.replace('  ', ' ') for s in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, the sixties. There were some interesting films. I was more of a movie goer then. I now enjoy renting movies and relaxing in my home rather than going to the theater. I also saw this short film, \" The Legend of the Boy and the Eagle\". I have been searching for this film for years. It was truly inspiring. Surprisingly, I was finally able to gather more information from your site. Thank You........ I'm surprised to find out that this short film was an opening for a Disney picture. I too did not remember the Disney film. I did not even remember that it was an opening film for Disney. I truly wish they would show this on TV sometime. I wonder if Disey holds the rights to this film? Is it available on DVD? This is a must see for all generations!!!\n"
     ]
    }
   ],
   "source": [
    "# Random sample from train dataset\n",
    "import random\n",
    "print (train_data[random.randint(0, len(train_data) - 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write the tokenization function \n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    return [token.text.lower() for token in tokens if (token.text not in punctuations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively try running the following multi-threaded version of tokenization\n",
    "# Credit to Ilya Kulikov\n",
    "def lower_case_remove_punc(parsed):\n",
    "    return [token.text.lower() for token in parsed if (token.text not in punctuations)]\n",
    "\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset\n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "\n",
    "    for sample in tqdm_notebook(tokenizer.pipe(dataset, disable=['parser', 'tagger', 'ner'], \n",
    "                                               batch_size=512, n_threads=1)):\n",
    "        tokens = lower_case_remove_punc(sample)\n",
    "\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens, all_train_tokens = tokenize_dataset(train_data)\n",
    "pkl.dump(train_data_tokens, open(\"train_data_tokens.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens, open(\"all_train_tokens.p\", \"wb\"))\n",
    "\n",
    "#val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens, _ = tokenize_dataset(val_data)\n",
    "pkl.dump(val_data_tokens, open(\"val_data_tokens.p\", \"wb\"))\n",
    "\n",
    "#test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens, _ = tokenize_dataset(test_data)\n",
    "pkl.dump(test_data_tokens, open(\"test_data_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No HTML\n",
    "#train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens_noHTML, all_train_tokens_noHTML = tokenize_dataset(train_data)\n",
    "pkl.dump(train_data_tokens_noHTML, open(\"train_data_tokens_noHTML.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens_noHTML, open(\"all_train_tokens_noHTML.p\", \"wb\"))\n",
    "\n",
    "#val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens_noHTML, _ = tokenize_dataset(val_data)\n",
    "pkl.dump(val_data_tokens_noHTML, open(\"val_data_tokens_noHTML.p\", \"wb\"))\n",
    "\n",
    "#test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens_noHTML, _ = tokenize_dataset(test_data)\n",
    "pkl.dump(test_data_tokens_noHTML, open(\"test_data_tokens_noHTML.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 4827560\n"
     ]
    }
   ],
   "source": [
    "# Then, load preprocessed train, val and test datasets\n",
    "train_data_tokens = pkl.load(open(\"train_data_tokens.p\", \"rb\"))\n",
    "all_train_tokens = pkl.load(open(\"all_train_tokens.p\", \"rb\"))\n",
    "\n",
    "val_data_tokens = pkl.load(open(\"val_data_tokens.p\", \"rb\"))\n",
    "test_data_tokens = pkl.load(open(\"test_data_tokens.p\", \"rb\"))\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 4780483\n"
     ]
    }
   ],
   "source": [
    "# No HTML\n",
    "# Then, load preprocessed train, val and test datasets\n",
    "train_data_tokens = pkl.load(open(\"train_data_tokens_noHTML.p\", \"rb\"))\n",
    "all_train_tokens = pkl.load(open(\"all_train_tokens_noHTML.p\", \"rb\"))\n",
    "\n",
    "val_data_tokens = pkl.load(open(\"val_data_tokens_noHTML.p\", \"rb\"))\n",
    "test_data_tokens = pkl.load(open(\"test_data_tokens_noHTML.p\", \"rb\"))\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 8187 ; token forgets\n",
      "Token forgets; token id 8187\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 500\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of IMDB tokens \n",
    "        @param target_list: list of IMDB targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def IMDB_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = IMDBDataset(train_data_indices, train_targets)\n",
    "#val_loader = IMDBDataset(val_data_indices, val_targets)\n",
    "#test_loader = IMDBDataset(test_data_indices, test_targets)\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "train_dataset = IMDBDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = IMDBDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = IMDBDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "# for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [101/400], Validation Acc: 70.24\n",
      "Epoch: [1/5], Step: [201/400], Validation Acc: 84.54\n",
      "Epoch: [1/5], Step: [301/400], Validation Acc: 86.72\n",
      "Epoch: [2/5], Step: [101/400], Validation Acc: 79.76\n",
      "Epoch: [2/5], Step: [201/400], Validation Acc: 73.12\n",
      "Epoch: [2/5], Step: [301/400], Validation Acc: 86.66\n",
      "Epoch: [3/5], Step: [101/400], Validation Acc: 85.96\n",
      "Epoch: [3/5], Step: [201/400], Validation Acc: 80.36\n",
      "Epoch: [3/5], Step: [301/400], Validation Acc: 79.72\n",
      "Epoch: [4/5], Step: [101/400], Validation Acc: 83.18\n",
      "Epoch: [4/5], Step: [201/400], Validation Acc: 80.58\n",
      "Epoch: [4/5], Step: [301/400], Validation Acc: 83.42\n",
      "Epoch: [5/5], Step: [101/400], Validation Acc: 87.74\n",
      "Epoch: [5/5], Step: [201/400], Validation Acc: 84.46\n",
      "Epoch: [5/5], Step: [301/400], Validation Acc: 83.72\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 5 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 5 epochs\n",
      "Val Acc 83.16\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-N-Grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization for n-grams\n",
    "def lower_case_remove_punc(parsed):\n",
    "    return [token.text.lower() for token in parsed if (token.text not in punctuations)]\n",
    "\n",
    "def tokenize_dataset(dataset,n):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset\n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "\n",
    "    for sample in tqdm_notebook(tokenizer.pipe(dataset, disable=['parser', 'tagger', 'ner'], \n",
    "                                               batch_size=512, n_threads=1)):\n",
    "        tokens = lower_case_remove_punc(sample)\n",
    "        n_gram_token = list(ngrams(tokens, n))\n",
    "        token_dataset.append(n_gram_token)\n",
    "        all_tokens += n_gram_token\n",
    "        \n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-grams ( not include 1-gram )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4691a46c6b254dca801786f053c6aabd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing val data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c855e6b81af430e96144687017cc4de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing test data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7569b6298745e9affb73190d192458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 2-grams without including 1-gram\n",
    "#train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens_2grams_alone, all_train_tokens_2grams_alone = tokenize_dataset(train_data,2)\n",
    "\n",
    "#all_train_tokens_2grams = all_train_tokens_2grams + all_train_tokens\n",
    "\n",
    "pkl.dump(train_data_tokens_2grams_alone, open(\"train_data_tokens_2grams_alone.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens_2grams_alone, open(\"all_train_tokens_2grams_alone.p\", \"wb\"))\n",
    "\n",
    "\n",
    "#val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens_2grams_alone, _ = tokenize_dataset(val_data,2)\n",
    "pkl.dump(val_data_tokens_2grams_alone, open(\"val_data_tokens_2grams_alone.p\", \"wb\"))\n",
    "\n",
    "#test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens_2grams_alone, _ = tokenize_dataset(test_data,2)\n",
    "pkl.dump(test_data_tokens_2grams_alone, open(\"test_data_tokens_2grams_alone.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 4765134\n"
     ]
    }
   ],
   "source": [
    "# Then, load preprocessed train, val and test datasets\n",
    "train_data_tokens_2grams_alone = pkl.load(open(\"train_data_tokens_2grams_alone.p\", \"rb\"))\n",
    "all_train_tokens_2grams_alone = pkl.load(open(\"all_train_tokens_2grams_alone.p\", \"rb\"))\n",
    "\n",
    "val_data_tokens_2grams_alone = pkl.load(open(\"val_data_tokens_2grams_alone.p\", \"rb\"))\n",
    "test_data_tokens_2grams_alone = pkl.load(open(\"test_data_tokens_2grams_alone.p\", \"rb\"))\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens_2grams_alone)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens_2grams_alone)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens_2grams_alone)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens_2grams_alone)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 50000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens_2grams_alone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 27578 ; Token ('people', 'so')\n",
      "Token ('people', 'so'); Token id 27578\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; Token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; Token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens_2grams_alone)\n",
    "val_data_indices = token2index_dataset(val_data_tokens_2grams_alone)\n",
    "test_data_indices = token2index_dataset(test_data_tokens_2grams_alone)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 500\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of IMDB tokens \n",
    "        @param target_list: list of IMDB targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def IMDB_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = IMDBDataset(train_data_indices, train_targets)\n",
    "#val_loader = IMDBDataset(val_data_indices, val_targets)\n",
    "#test_loader = IMDBDataset(test_data_indices, test_targets)\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "train_dataset = IMDBDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = IMDBDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = IMDBDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "# for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [101/400], Validation Acc: 29.04\n",
      "Epoch: [1/5], Step: [201/400], Validation Acc: 73.6\n",
      "Epoch: [1/5], Step: [301/400], Validation Acc: 79.32\n",
      "Epoch: [2/5], Step: [101/400], Validation Acc: 79.42\n",
      "Epoch: [2/5], Step: [201/400], Validation Acc: 76.42\n",
      "Epoch: [2/5], Step: [301/400], Validation Acc: 83.34\n",
      "Epoch: [3/5], Step: [101/400], Validation Acc: 82.64\n",
      "Epoch: [3/5], Step: [201/400], Validation Acc: 79.82\n",
      "Epoch: [3/5], Step: [301/400], Validation Acc: 79.26\n",
      "Epoch: [4/5], Step: [101/400], Validation Acc: 82.7\n",
      "Epoch: [4/5], Step: [201/400], Validation Acc: 82.78\n",
      "Epoch: [4/5], Step: [301/400], Validation Acc: 82.66\n",
      "Epoch: [5/5], Step: [101/400], Validation Acc: 81.86\n",
      "Epoch: [5/5], Step: [201/400], Validation Acc: 82.48\n",
      "Epoch: [5/5], Step: [301/400], Validation Acc: 83.28\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 5 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 5 epochs\n",
      "Val Acc 81.78\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-grams ( include 1-gram )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce469c609023404f8b95b174c151aa0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing val data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96fc1e2281d0491aba626baa96f26b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing test data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9e0d1c072054634a9e516d850139bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 2-grams\n",
    "#train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens_2grams, all_train_tokens_2grams = tokenize_dataset(train_data,2)\n",
    "\n",
    "all_train_tokens_2grams = all_train_tokens_2grams + all_train_tokens\n",
    "\n",
    "pkl.dump(train_data_tokens_2grams, open(\"train_data_tokens_2grams.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens_2grams, open(\"all_train_tokens_2grams.p\", \"wb\"))\n",
    "\n",
    "\n",
    "#val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens_2grams, _ = tokenize_dataset(val_data,2)\n",
    "pkl.dump(val_data_tokens_2grams, open(\"val_data_tokens_2grams.p\", \"wb\"))\n",
    "\n",
    "#test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens_2grams, _ = tokenize_dataset(test_data,2)\n",
    "pkl.dump(test_data_tokens_2grams, open(\"test_data_tokens_2grams.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 9545617\n"
     ]
    }
   ],
   "source": [
    "# Then, load preprocessed train, val and test datasets\n",
    "train_data_tokens_2grams = pkl.load(open(\"train_data_tokens_2grams.p\", \"rb\"))\n",
    "all_train_tokens_2grams = pkl.load(open(\"all_train_tokens_2grams.p\", \"rb\"))\n",
    "\n",
    "val_data_tokens_2grams = pkl.load(open(\"val_data_tokens_2grams.p\", \"rb\"))\n",
    "test_data_tokens_2grams = pkl.load(open(\"test_data_tokens_2grams.p\", \"rb\"))\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens_2grams)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens_2grams)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens_2grams)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens_2grams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2id_2grams, id2token_2grams = build_vocab(all_train_tokens_2grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 11346 ; Token ('save', 'this')\n",
      "Token ('save', 'this'); Token id 11346\n"
     ]
    }
   ],
   "source": [
    "random_token_id = randint(0, len(id2token_2grams)-1)\n",
    "random_token = id2token_2grams[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; Token {}\".format(random_token_id, id2token_2grams[random_token_id]))\n",
    "print (\"Token {}; Token id {}\".format(random_token, token2id_2grams[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id_2grams[token] if token in token2id_2grams else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices_2grams = token2index_dataset(train_data_tokens_2grams)\n",
    "val_data_indices_2grams = token2index_dataset(val_data_tokens_2grams)\n",
    "test_data_indices_2grams = token2index_dataset(test_data_tokens_2grams)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices_2grams)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices_2grams)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices_2grams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 500\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of IMDB tokens \n",
    "        @param target_list: list of IMDB targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def IMDB_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = IMDBDataset(train_data_indices, train_targets)\n",
    "#val_loader = IMDBDataset(val_data_indices, val_targets)\n",
    "#test_loader = IMDBDataset(test_data_indices, test_targets)\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "train_dataset = IMDBDataset(train_data_indices_2grams, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = IMDBDataset(val_data_indices_2grams, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = IMDBDataset(test_data_indices_2grams, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "# for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token_2grams), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [101/400], Validation Acc: 26.74\n",
      "Epoch: [1/5], Step: [201/400], Validation Acc: 77.38\n",
      "Epoch: [1/5], Step: [301/400], Validation Acc: 65.54\n",
      "Epoch: [2/5], Step: [101/400], Validation Acc: 78.22\n",
      "Epoch: [2/5], Step: [201/400], Validation Acc: 75.5\n",
      "Epoch: [2/5], Step: [301/400], Validation Acc: 82.0\n",
      "Epoch: [3/5], Step: [101/400], Validation Acc: 77.54\n",
      "Epoch: [3/5], Step: [201/400], Validation Acc: 79.52\n",
      "Epoch: [3/5], Step: [301/400], Validation Acc: 78.96\n",
      "Epoch: [4/5], Step: [101/400], Validation Acc: 84.98\n",
      "Epoch: [4/5], Step: [201/400], Validation Acc: 83.24\n",
      "Epoch: [4/5], Step: [301/400], Validation Acc: 84.08\n",
      "Epoch: [5/5], Step: [101/400], Validation Acc: 81.88\n",
      "Epoch: [5/5], Step: [201/400], Validation Acc: 81.12\n",
      "Epoch: [5/5], Step: [301/400], Validation Acc: 81.8\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 5 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 5 epochs\n",
      "Val Acc 82.7\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vary N to generate Bag-of-N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-grams\n",
    "#train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens_2grams, all_train_tokens_2grams = tokenize_dataset(train_data,2)\n",
    "\n",
    "all_train_tokens_2grams = all_train_tokens_2grams + all_train_tokens\n",
    "\n",
    "pkl.dump(train_data_tokens_2grams, open(\"train_data_tokens_2grams.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens_2grams, open(\"all_train_tokens_2grams.p\", \"wb\"))\n",
    "\n",
    "\n",
    "#val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens_2grams, _ = tokenize_dataset(val_data,2)\n",
    "pkl.dump(val_data_tokens_2grams, open(\"val_data_tokens_2grams.p\", \"wb\"))\n",
    "\n",
    "#test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens_2grams, _ = tokenize_dataset(test_data,2)\n",
    "pkl.dump(test_data_tokens_2grams, open(\"test_data_tokens_2grams.p\", \"wb\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 9545617\n",
      "Token id 29720 ; Token ('of', 'originality')\n",
      "Token ('of', 'originality'); Token id 29720\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# Then, load preprocessed train, val and test datasets\n",
    "train_data_tokens_2grams = pkl.load(open(\"train_data_tokens_2grams.p\", \"rb\"))\n",
    "all_train_tokens_2grams = pkl.load(open(\"all_train_tokens_2grams.p\", \"rb\"))\n",
    "\n",
    "val_data_tokens_2grams = pkl.load(open(\"val_data_tokens_2grams.p\", \"rb\"))\n",
    "test_data_tokens_2grams = pkl.load(open(\"test_data_tokens_2grams.p\", \"rb\"))\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens_2grams)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens_2grams)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens_2grams)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens_2grams)))\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens_2grams)\n",
    "\n",
    "# Lets check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; Token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; Token id {}\".format(random_token, token2id[random_token]))\n",
    "\n",
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens_2grams)\n",
    "val_data_indices = token2index_dataset(val_data_tokens_2grams)\n",
    "test_data_indices = token2index_dataset(test_data_tokens_2grams)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 500\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of IMDB tokens \n",
    "        @param target_list: list of IMDB targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def IMDB_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = IMDBDataset(train_data_indices, train_targets)\n",
    "#val_loader = IMDBDataset(val_data_indices, val_targets)\n",
    "#test_loader = IMDBDataset(test_data_indices, test_targets)\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "train_dataset = IMDBDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = IMDBDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = IMDBDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "# for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [101/400], Validation Acc: 54.3\n",
      "Epoch: [1/5], Step: [201/400], Validation Acc: 75.06\n",
      "Epoch: [1/5], Step: [301/400], Validation Acc: 69.42\n",
      "Epoch: [2/5], Step: [101/400], Validation Acc: 83.52\n",
      "Epoch: [2/5], Step: [201/400], Validation Acc: 76.72\n",
      "Epoch: [2/5], Step: [301/400], Validation Acc: 87.6\n",
      "Epoch: [3/5], Step: [101/400], Validation Acc: 81.12\n",
      "Epoch: [3/5], Step: [201/400], Validation Acc: 82.68\n",
      "Epoch: [3/5], Step: [301/400], Validation Acc: 88.56\n",
      "Epoch: [4/5], Step: [101/400], Validation Acc: 82.42\n",
      "Epoch: [4/5], Step: [201/400], Validation Acc: 81.28\n",
      "Epoch: [4/5], Step: [301/400], Validation Acc: 80.0\n",
      "Epoch: [5/5], Step: [101/400], Validation Acc: 82.32\n",
      "Epoch: [5/5], Step: [201/400], Validation Acc: 82.16\n",
      "Epoch: [5/5], Step: [301/400], Validation Acc: 80.62\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 5 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 5 epochs\n",
      "Val Acc 81.36\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-grams\n",
    "#train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens_3grams, all_train_tokens_3grams = tokenize_dataset(train_data,3)\n",
    "\n",
    "all_train_tokens_3grams = all_train_tokens_3grams + all_train_tokens_2grams\n",
    "\n",
    "pkl.dump(train_data_tokens_3grams, open(\"train_data_tokens_3grams.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens_3grams, open(\"all_train_tokens_3grams.p\", \"wb\"))\n",
    "\n",
    "\n",
    "#val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens_3grams, _ = tokenize_dataset(val_data,3)\n",
    "\n",
    "pkl.dump(val_data_tokens_3grams, open(\"val_data_tokens_3grams.p\", \"wb\"))\n",
    "\n",
    "#test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens_3grams, _ = tokenize_dataset(test_data,3)\n",
    "\n",
    "pkl.dump(test_data_tokens_3grams, open(\"test_data_tokens_3grams.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 14422680\n"
     ]
    }
   ],
   "source": [
    "# Then, load preprocessed train, val and test datasets\n",
    "train_data_tokens_3grams = pkl.load(open(\"train_data_tokens_3grams.p\", \"rb\"))\n",
    "all_train_tokens_3grams = pkl.load(open(\"all_train_tokens_3grams.p\", \"rb\"))\n",
    "\n",
    "val_data_tokens_3grams = pkl.load(open(\"val_data_tokens_3grams.p\", \"rb\"))\n",
    "test_data_tokens_3grams = pkl.load(open(\"test_data_tokens_3grams.p\", \"rb\"))\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens_3grams)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens_3grams)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens_3grams)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens_3grams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 24901 ; Token ('acts', 'as')\n",
      "Token ('acts', 'as'); Token id 24901\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "token2id, id2token = build_vocab(all_train_tokens_3grams)\n",
    "\n",
    "# Lets check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; Token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; Token id {}\".format(random_token, token2id[random_token]))\n",
    "\n",
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens_3grams)\n",
    "val_data_indices = token2index_dataset(val_data_tokens_3grams)\n",
    "test_data_indices = token2index_dataset(test_data_tokens_3grams)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 500\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of IMDB tokens \n",
    "        @param target_list: list of IMDB targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def IMDB_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = IMDBDataset(train_data_indices, train_targets)\n",
    "#val_loader = IMDBDataset(val_data_indices, val_targets)\n",
    "#test_loader = IMDBDataset(test_data_indices, test_targets)\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "train_dataset = IMDBDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = IMDBDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = IMDBDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "# for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [101/400], Validation Acc: 7.74\n",
      "Epoch: [1/5], Step: [201/400], Validation Acc: 12.2\n",
      "Epoch: [1/5], Step: [301/400], Validation Acc: 64.1\n",
      "Epoch: [2/5], Step: [101/400], Validation Acc: 75.76\n",
      "Epoch: [2/5], Step: [201/400], Validation Acc: 60.92\n",
      "Epoch: [2/5], Step: [301/400], Validation Acc: 70.28\n",
      "Epoch: [3/5], Step: [101/400], Validation Acc: 73.3\n",
      "Epoch: [3/5], Step: [201/400], Validation Acc: 65.38\n",
      "Epoch: [3/5], Step: [301/400], Validation Acc: 68.48\n",
      "Epoch: [4/5], Step: [101/400], Validation Acc: 65.4\n",
      "Epoch: [4/5], Step: [201/400], Validation Acc: 70.0\n",
      "Epoch: [4/5], Step: [301/400], Validation Acc: 73.74\n",
      "Epoch: [5/5], Step: [101/400], Validation Acc: 73.46\n",
      "Epoch: [5/5], Step: [201/400], Validation Acc: 76.66\n",
      "Epoch: [5/5], Step: [301/400], Validation Acc: 69.64\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 5 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 5 epochs\n",
      "Val Acc 64.9\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-grams\n",
    "#train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens_4grams, all_train_tokens_4grams = tokenize_dataset(train_data,4)\n",
    "\n",
    "all_train_tokens_4grams = all_train_tokens_4grams + all_train_tokens_3grams\n",
    "\n",
    "pkl.dump(train_data_tokens_4grams, open(\"train_data_tokens_4grams.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens_4grams, open(\"all_train_tokens_4grams.p\", \"wb\"))\n",
    "\n",
    "\n",
    "#val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens_4grams, _ = tokenize_dataset(val_data,4)\n",
    "\n",
    "pkl.dump(val_data_tokens_4grams, open(\"val_data_tokens_4grams.p\", \"wb\"))\n",
    "\n",
    "#test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens_4grams, _ = tokenize_dataset(test_data,4)\n",
    "\n",
    "pkl.dump(test_data_tokens_4grams, open(\"test_data_tokens_4grams.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 19190240\n"
     ]
    }
   ],
   "source": [
    "# Then, load preprocessed train, val and test datasets\n",
    "train_data_tokens_4grams = pkl.load(open(\"train_data_tokens_4grams.p\", \"rb\"))\n",
    "all_train_tokens_4grams = pkl.load(open(\"all_train_tokens_4grams.p\", \"rb\"))\n",
    "\n",
    "val_data_tokens_4grams = pkl.load(open(\"val_data_tokens_4grams.p\", \"rb\"))\n",
    "test_data_tokens_4grams = pkl.load(open(\"test_data_tokens_4grams.p\", \"rb\"))\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens_4grams)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens_4grams)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens_4grams)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens_4grams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2id, id2token = build_vocab(all_train_tokens_4grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 22785 ; Token ('year', 'i')\n",
      "Token ('year', 'i'); Token id 22785\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; Token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; Token id {}\".format(random_token, token2id[random_token]))\n",
    "\n",
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens_4grams)\n",
    "val_data_indices = token2index_dataset(val_data_tokens_4grams)\n",
    "test_data_indices = token2index_dataset(test_data_tokens_4grams)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 500\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of IMDB tokens \n",
    "        @param target_list: list of IMDB targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def IMDB_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = IMDBDataset(train_data_indices, train_targets)\n",
    "#val_loader = IMDBDataset(val_data_indices, val_targets)\n",
    "#test_loader = IMDBDataset(test_data_indices, test_targets)\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "train_dataset = IMDBDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = IMDBDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = IMDBDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "# for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [101/400], Validation Acc: 57.06\n",
      "Epoch: [1/5], Step: [201/400], Validation Acc: 54.92\n",
      "Epoch: [1/5], Step: [301/400], Validation Acc: 58.9\n",
      "Epoch: [2/5], Step: [101/400], Validation Acc: 48.54\n",
      "Epoch: [2/5], Step: [201/400], Validation Acc: 48.72\n",
      "Epoch: [2/5], Step: [301/400], Validation Acc: 57.38\n",
      "Epoch: [3/5], Step: [101/400], Validation Acc: 56.74\n",
      "Epoch: [3/5], Step: [201/400], Validation Acc: 78.6\n",
      "Epoch: [3/5], Step: [301/400], Validation Acc: 48.42\n",
      "Epoch: [4/5], Step: [101/400], Validation Acc: 47.6\n",
      "Epoch: [4/5], Step: [201/400], Validation Acc: 61.1\n",
      "Epoch: [4/5], Step: [301/400], Validation Acc: 52.9\n",
      "Epoch: [5/5], Step: [101/400], Validation Acc: 55.76\n",
      "Epoch: [5/5], Step: [201/400], Validation Acc: 52.92\n",
      "Epoch: [5/5], Step: [301/400], Validation Acc: 53.12\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 5 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 5 epochs\n",
      "Val Acc 52.52\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-2-Grams model\n",
    "### Vary the vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, load preprocessed train, val and test datasets\n",
    "train_data_tokens_2grams = pkl.load(open(\"train_data_tokens_2grams.p\", \"rb\"))\n",
    "all_train_tokens_2grams = pkl.load(open(\"all_train_tokens_2grams.p\", \"rb\"))\n",
    "\n",
    "val_data_tokens_2grams = pkl.load(open(\"val_data_tokens_2grams.p\", \"rb\"))\n",
    "test_data_tokens_2grams = pkl.load(open(\"test_data_tokens_2grams.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = [10000,15000,20000,25000,30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Vocabulary Size =  10000\n",
      "Epoch: [1/5], Step: [101/400], Validation Acc: 83.1\n",
      "Epoch: [1/5], Step: [201/400], Validation Acc: 72.72\n",
      "Epoch: [1/5], Step: [301/400], Validation Acc: 70.54\n",
      "Epoch: [2/5], Step: [101/400], Validation Acc: 72.34\n",
      "Epoch: [2/5], Step: [201/400], Validation Acc: 80.1\n",
      "Epoch: [2/5], Step: [301/400], Validation Acc: 75.36\n",
      "Epoch: [3/5], Step: [101/400], Validation Acc: 73.56\n",
      "Epoch: [3/5], Step: [201/400], Validation Acc: 80.48\n",
      "Epoch: [3/5], Step: [301/400], Validation Acc: 76.02\n",
      "Epoch: [4/5], Step: [101/400], Validation Acc: 68.86\n",
      "Epoch: [4/5], Step: [201/400], Validation Acc: 77.34\n",
      "Epoch: [4/5], Step: [301/400], Validation Acc: 77.94\n",
      "Epoch: [5/5], Step: [101/400], Validation Acc: 74.0\n",
      "Epoch: [5/5], Step: [201/400], Validation Acc: 75.28\n",
      "Epoch: [5/5], Step: [301/400], Validation Acc: 75.66\n",
      "After training for 5 epochs\n",
      "Val Acc 71.78\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "Vocabulary Size =  15000\n",
      "Epoch: [1/5], Step: [101/400], Validation Acc: 53.04\n",
      "Epoch: [1/5], Step: [201/400], Validation Acc: 77.26\n",
      "Epoch: [1/5], Step: [301/400], Validation Acc: 84.62\n",
      "Epoch: [2/5], Step: [101/400], Validation Acc: 79.1\n",
      "Epoch: [2/5], Step: [201/400], Validation Acc: 82.5\n",
      "Epoch: [2/5], Step: [301/400], Validation Acc: 84.58\n",
      "Epoch: [3/5], Step: [101/400], Validation Acc: 80.22\n",
      "Epoch: [3/5], Step: [201/400], Validation Acc: 79.7\n",
      "Epoch: [3/5], Step: [301/400], Validation Acc: 82.94\n",
      "Epoch: [4/5], Step: [101/400], Validation Acc: 81.68\n",
      "Epoch: [4/5], Step: [201/400], Validation Acc: 77.74\n",
      "Epoch: [4/5], Step: [301/400], Validation Acc: 77.14\n",
      "Epoch: [5/5], Step: [101/400], Validation Acc: 78.68\n",
      "Epoch: [5/5], Step: [201/400], Validation Acc: 73.96\n",
      "Epoch: [5/5], Step: [301/400], Validation Acc: 81.94\n",
      "After training for 5 epochs\n",
      "Val Acc 76.66\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "Vocabulary Size =  20000\n",
      "Epoch: [1/5], Step: [101/400], Validation Acc: 59.06\n",
      "Epoch: [1/5], Step: [201/400], Validation Acc: 63.62\n",
      "Epoch: [1/5], Step: [301/400], Validation Acc: 89.22\n",
      "Epoch: [2/5], Step: [101/400], Validation Acc: 85.94\n",
      "Epoch: [2/5], Step: [201/400], Validation Acc: 84.32\n",
      "Epoch: [2/5], Step: [301/400], Validation Acc: 78.7\n",
      "Epoch: [3/5], Step: [101/400], Validation Acc: 81.96\n",
      "Epoch: [3/5], Step: [201/400], Validation Acc: 75.28\n",
      "Epoch: [3/5], Step: [301/400], Validation Acc: 78.12\n",
      "Epoch: [4/5], Step: [101/400], Validation Acc: 81.66\n",
      "Epoch: [4/5], Step: [201/400], Validation Acc: 80.42\n",
      "Epoch: [4/5], Step: [301/400], Validation Acc: 78.46\n",
      "Epoch: [5/5], Step: [101/400], Validation Acc: 80.56\n",
      "Epoch: [5/5], Step: [201/400], Validation Acc: 73.0\n",
      "Epoch: [5/5], Step: [301/400], Validation Acc: 73.78\n",
      "After training for 5 epochs\n",
      "Val Acc 74.18\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "Vocabulary Size =  25000\n",
      "Epoch: [1/5], Step: [101/400], Validation Acc: 30.68\n",
      "Epoch: [1/5], Step: [201/400], Validation Acc: 73.82\n",
      "Epoch: [1/5], Step: [301/400], Validation Acc: 86.6\n",
      "Epoch: [2/5], Step: [101/400], Validation Acc: 82.82\n",
      "Epoch: [2/5], Step: [201/400], Validation Acc: 77.6\n",
      "Epoch: [2/5], Step: [301/400], Validation Acc: 74.96\n",
      "Epoch: [3/5], Step: [101/400], Validation Acc: 84.52\n",
      "Epoch: [3/5], Step: [201/400], Validation Acc: 82.32\n",
      "Epoch: [3/5], Step: [301/400], Validation Acc: 80.44\n",
      "Epoch: [4/5], Step: [101/400], Validation Acc: 81.48\n",
      "Epoch: [4/5], Step: [201/400], Validation Acc: 80.16\n",
      "Epoch: [4/5], Step: [301/400], Validation Acc: 83.58\n",
      "Epoch: [5/5], Step: [101/400], Validation Acc: 82.14\n",
      "Epoch: [5/5], Step: [201/400], Validation Acc: 82.86\n",
      "Epoch: [5/5], Step: [301/400], Validation Acc: 82.54\n",
      "After training for 5 epochs\n",
      "Val Acc 80.6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "Vocabulary Size =  30000\n",
      "Epoch: [1/5], Step: [101/400], Validation Acc: 29.96\n",
      "Epoch: [1/5], Step: [201/400], Validation Acc: 71.26\n",
      "Epoch: [1/5], Step: [301/400], Validation Acc: 73.1\n",
      "Epoch: [2/5], Step: [101/400], Validation Acc: 76.9\n",
      "Epoch: [2/5], Step: [201/400], Validation Acc: 67.26\n",
      "Epoch: [2/5], Step: [301/400], Validation Acc: 84.92\n",
      "Epoch: [3/5], Step: [101/400], Validation Acc: 84.56\n",
      "Epoch: [3/5], Step: [201/400], Validation Acc: 80.36\n",
      "Epoch: [3/5], Step: [301/400], Validation Acc: 83.82\n",
      "Epoch: [4/5], Step: [101/400], Validation Acc: 80.54\n",
      "Epoch: [4/5], Step: [201/400], Validation Acc: 80.6\n",
      "Epoch: [4/5], Step: [301/400], Validation Acc: 82.06\n",
      "Epoch: [5/5], Step: [101/400], Validation Acc: 80.98\n",
      "Epoch: [5/5], Step: [201/400], Validation Acc: 81.26\n",
      "Epoch: [5/5], Step: [301/400], Validation Acc: 81.2\n",
      "After training for 5 epochs\n",
      "Val Acc 82.08\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for i in range(len(size)):\n",
    "    max_vocab_size = int(size[i])\n",
    "    PAD_IDX = 0\n",
    "    UNK_IDX = 1\n",
    "    token2id, id2token = build_vocab(all_train_tokens_2grams)\n",
    "    \n",
    "    # convert token to id in the dataset\n",
    "    train_data_indices = token2index_dataset(train_data_tokens_2grams)\n",
    "    val_data_indices = token2index_dataset(val_data_tokens_2grams)\n",
    "    test_data_indices = token2index_dataset(test_data_tokens_2grams)\n",
    "    \n",
    "    train_dataset = IMDBDataset(train_data_indices, train_targets)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "    val_dataset = IMDBDataset(val_data_indices, val_targets)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=IMDB_collate_func,\n",
    "                                               shuffle=True)\n",
    "\n",
    "    test_dataset = IMDBDataset(test_data_indices, test_targets)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=IMDB_collate_func,\n",
    "                                               shuffle=False)\n",
    "    emb_dim = 100\n",
    "    model = BagOfWords(len(id2token), emb_dim)\n",
    "    learning_rate = 0.01\n",
    "    num_epochs = 5 # number epoch to train\n",
    "    criterion = torch.nn.CrossEntropyLoss()  \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print(\"-\"*20)\n",
    "    print('Vocabulary Size = ',size[i])\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data, lengths, labels\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                # validate\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "    print (\"After training for {} epochs\".format(num_epochs))\n",
    "    print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "    print('\\n'*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-2-Grams model\n",
    "### Vocabulary Size = 30000\n",
    "### Vary the embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = [100,300,500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 30000\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "        token_counter = Counter(all_tokens)\n",
    "        vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "        id2token = list(vocab)\n",
    "        token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "        id2token = ['<pad>', '<unk>'] + id2token\n",
    "        token2id['<pad>'] = PAD_IDX \n",
    "        token2id['<unk>'] = UNK_IDX\n",
    "        return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens_2grams)\n",
    "\n",
    "# convert token to id in the dataset\n",
    "train_data_indices = token2index_dataset(train_data_tokens_2grams)\n",
    "val_data_indices = token2index_dataset(val_data_tokens_2grams)\n",
    "test_data_indices = token2index_dataset(test_data_tokens_2grams)\n",
    "\n",
    "train_dataset = IMDBDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                       batch_size=BATCH_SIZE,\n",
    "                                       collate_fn=IMDB_collate_func,\n",
    "                                       shuffle=True)\n",
    "\n",
    "val_dataset = IMDBDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = IMDBDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=False)\n",
    "for i in range(len(size)):\n",
    "    emb_dim = 0.05\n",
    "    model = BagOfWords(len(id2token), emb_dim)\n",
    "    learning_rate = 0.01\n",
    "    num_epochs = 5 # number epoch to train\n",
    "    criterion = torch.nn.CrossEntropyLoss()  \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(\"-\"*20)\n",
    "    print('Embedding Size = ',size[i])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data, lengths, labels\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                # validate\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "    print (\"After training for {} epochs\".format(num_epochs))\n",
    "    print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "    print('\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-2-Grams model\n",
    "### Vocabulary Size = 30000\n",
    "### Embedding Size = 100\n",
    "### Vary optimization hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [101/400], Validation Acc: 50.4\n",
      "Epoch: [1/5], Step: [201/400], Validation Acc: 66.56\n",
      "Epoch: [1/5], Step: [301/400], Validation Acc: 85.34\n",
      "Epoch: [2/5], Step: [101/400], Validation Acc: 72.04\n",
      "Epoch: [2/5], Step: [201/400], Validation Acc: 82.98\n",
      "Epoch: [2/5], Step: [301/400], Validation Acc: 82.48\n",
      "Epoch: [3/5], Step: [101/400], Validation Acc: 74.0\n",
      "Epoch: [3/5], Step: [201/400], Validation Acc: 84.04\n",
      "Epoch: [3/5], Step: [301/400], Validation Acc: 88.28\n",
      "Epoch: [4/5], Step: [101/400], Validation Acc: 84.6\n",
      "Epoch: [4/5], Step: [201/400], Validation Acc: 85.7\n",
      "Epoch: [4/5], Step: [301/400], Validation Acc: 82.74\n",
      "Epoch: [5/5], Step: [101/400], Validation Acc: 79.86\n",
      "Epoch: [5/5], Step: [201/400], Validation Acc: 83.88\n",
      "Epoch: [5/5], Step: [301/400], Validation Acc: 79.36\n",
      "After training for 5 epochs\n",
      "By using Adam\n",
      "Val Acc 80.78\n"
     ]
    }
   ],
   "source": [
    "max_vocab_size = 30000\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "        token_counter = Counter(all_tokens)\n",
    "        vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "        id2token = list(vocab)\n",
    "        token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "        id2token = ['<pad>', '<unk>'] + id2token\n",
    "        token2id['<pad>'] = PAD_IDX \n",
    "        token2id['<unk>'] = UNK_IDX\n",
    "        return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens_2grams)\n",
    "\n",
    "# convert token to id in the dataset\n",
    "train_data_indices = token2index_dataset(train_data_tokens_2grams)\n",
    "val_data_indices = token2index_dataset(val_data_tokens_2grams)\n",
    "test_data_indices = token2index_dataset(test_data_tokens_2grams)\n",
    "\n",
    "train_dataset = IMDBDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                       batch_size=BATCH_SIZE,\n",
    "                                       collate_fn=IMDB_collate_func,\n",
    "                                       shuffle=True)\n",
    "\n",
    "val_dataset = IMDBDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = IMDBDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "learning_rate = 0.01\n",
    "num_epochs = 5 # number epoch to train\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print('By using Adam')\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vary Learning Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = [0.01, 0.005, 0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Learning Rate =  0.01\n",
      "Epoch: [1/5], Step: [101/400], Validation Acc: 59.66\n",
      "Epoch: [1/5], Step: [201/400], Validation Acc: 62.86\n",
      "Epoch: [1/5], Step: [301/400], Validation Acc: 83.2\n",
      "Epoch: [2/5], Step: [101/400], Validation Acc: 85.2\n",
      "Epoch: [2/5], Step: [201/400], Validation Acc: 89.2\n",
      "Epoch: [2/5], Step: [301/400], Validation Acc: 84.36\n",
      "Epoch: [3/5], Step: [101/400], Validation Acc: 79.84\n",
      "Epoch: [3/5], Step: [201/400], Validation Acc: 81.54\n",
      "Epoch: [3/5], Step: [301/400], Validation Acc: 77.54\n",
      "Epoch: [4/5], Step: [101/400], Validation Acc: 79.48\n",
      "Epoch: [4/5], Step: [201/400], Validation Acc: 83.12\n",
      "Epoch: [4/5], Step: [301/400], Validation Acc: 82.04\n",
      "Epoch: [5/5], Step: [101/400], Validation Acc: 81.54\n",
      "Epoch: [5/5], Step: [201/400], Validation Acc: 78.7\n",
      "Epoch: [5/5], Step: [301/400], Validation Acc: 79.84\n",
      "After training for 5 epochs\n",
      "Val Acc 78.12\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "Learning Rate =  0.005\n",
      "Epoch: [1/5], Step: [101/400], Validation Acc: 77.76\n",
      "Epoch: [1/5], Step: [201/400], Validation Acc: 80.18\n",
      "Epoch: [1/5], Step: [301/400], Validation Acc: 83.26\n",
      "Epoch: [2/5], Step: [101/400], Validation Acc: 81.8\n",
      "Epoch: [2/5], Step: [201/400], Validation Acc: 80.94\n",
      "Epoch: [2/5], Step: [301/400], Validation Acc: 82.46\n",
      "Epoch: [3/5], Step: [101/400], Validation Acc: 81.28\n",
      "Epoch: [3/5], Step: [201/400], Validation Acc: 79.6\n",
      "Epoch: [3/5], Step: [301/400], Validation Acc: 80.22\n",
      "Epoch: [4/5], Step: [101/400], Validation Acc: 81.62\n",
      "Epoch: [4/5], Step: [201/400], Validation Acc: 80.78\n",
      "Epoch: [4/5], Step: [301/400], Validation Acc: 79.86\n",
      "Epoch: [5/5], Step: [101/400], Validation Acc: 80.22\n",
      "Epoch: [5/5], Step: [201/400], Validation Acc: 80.34\n",
      "Epoch: [5/5], Step: [301/400], Validation Acc: 81.04\n",
      "After training for 5 epochs\n",
      "Val Acc 80.52\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "Learning Rate =  0.001\n",
      "Epoch: [1/5], Step: [101/400], Validation Acc: 81.14\n",
      "Epoch: [1/5], Step: [201/400], Validation Acc: 80.36\n",
      "Epoch: [1/5], Step: [301/400], Validation Acc: 80.7\n",
      "Epoch: [2/5], Step: [101/400], Validation Acc: 80.4\n",
      "Epoch: [2/5], Step: [201/400], Validation Acc: 80.44\n",
      "Epoch: [2/5], Step: [301/400], Validation Acc: 80.54\n",
      "Epoch: [3/5], Step: [101/400], Validation Acc: 80.62\n",
      "Epoch: [3/5], Step: [201/400], Validation Acc: 80.98\n",
      "Epoch: [3/5], Step: [301/400], Validation Acc: 80.38\n",
      "Epoch: [4/5], Step: [101/400], Validation Acc: 80.62\n",
      "Epoch: [4/5], Step: [201/400], Validation Acc: 80.22\n",
      "Epoch: [4/5], Step: [301/400], Validation Acc: 80.66\n",
      "Epoch: [5/5], Step: [101/400], Validation Acc: 80.82\n",
      "Epoch: [5/5], Step: [201/400], Validation Acc: 80.94\n",
      "Epoch: [5/5], Step: [301/400], Validation Acc: 80.7\n",
      "After training for 5 epochs\n",
      "Val Acc 80.5\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_vocab_size = 30000\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "        token_counter = Counter(all_tokens)\n",
    "        vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "        id2token = list(vocab)\n",
    "        token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "        id2token = ['<pad>', '<unk>'] + id2token\n",
    "        token2id['<pad>'] = PAD_IDX \n",
    "        token2id['<unk>'] = UNK_IDX\n",
    "        return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens_2grams)\n",
    "\n",
    "# convert token to id in the dataset\n",
    "train_data_indices = token2index_dataset(train_data_tokens_2grams)\n",
    "val_data_indices = token2index_dataset(val_data_tokens_2grams)\n",
    "test_data_indices = token2index_dataset(test_data_tokens_2grams)\n",
    "\n",
    "train_dataset = IMDBDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                       batch_size=BATCH_SIZE,\n",
    "                                       collate_fn=IMDB_collate_func,\n",
    "                                       shuffle=True)\n",
    "\n",
    "val_dataset = IMDBDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = IMDBDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=False)\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "\n",
    "\n",
    "for i in range(len(rate)):\n",
    "    learning_rate = rate[i]\n",
    "    num_epochs = 5 # number epoch to train\n",
    "    criterion = torch.nn.CrossEntropyLoss()  \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    print(\"-\"*20)\n",
    "    print('Learning Rate = ',rate[i])\n",
    "    for epoch in range(num_epochs):\n",
    "        sche\n",
    "        for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data, lengths, labels\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                # validate\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "    print (\"After training for {} epochs\".format(num_epochs))\n",
    "    print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "    print('\\n'*2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [101/400], Validation Acc: 0.02\n",
      "Epoch: [1/5], Step: [201/400], Validation Acc: 0.04\n",
      "Epoch: [1/5], Step: [301/400], Validation Acc: 0.08\n",
      "Epoch: [2/5], Step: [101/400], Validation Acc: 2.0\n",
      "Epoch: [2/5], Step: [201/400], Validation Acc: 4.5\n",
      "Epoch: [2/5], Step: [301/400], Validation Acc: 24.72\n",
      "Epoch: [3/5], Step: [101/400], Validation Acc: 33.76\n",
      "Epoch: [3/5], Step: [201/400], Validation Acc: 61.96\n",
      "Epoch: [3/5], Step: [301/400], Validation Acc: 62.6\n",
      "Epoch: [4/5], Step: [101/400], Validation Acc: 66.9\n",
      "Epoch: [4/5], Step: [201/400], Validation Acc: 71.6\n",
      "Epoch: [4/5], Step: [301/400], Validation Acc: 71.96\n",
      "Epoch: [5/5], Step: [101/400], Validation Acc: 66.62\n",
      "Epoch: [5/5], Step: [201/400], Validation Acc: 77.8\n",
      "Epoch: [5/5], Step: [301/400], Validation Acc: 79.34\n",
      "After training for 5 epochs\n",
      "Val Acc 78.84\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_vocab_size = 30000\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "        token_counter = Counter(all_tokens)\n",
    "        vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "        id2token = list(vocab)\n",
    "        token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "        id2token = ['<pad>', '<unk>'] + id2token\n",
    "        token2id['<pad>'] = PAD_IDX \n",
    "        token2id['<unk>'] = UNK_IDX\n",
    "        return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens_2grams)\n",
    "\n",
    "# convert token to id in the dataset\n",
    "train_data_indices = token2index_dataset(train_data_tokens_2grams)\n",
    "val_data_indices = token2index_dataset(val_data_tokens_2grams)\n",
    "test_data_indices = token2index_dataset(test_data_tokens_2grams)\n",
    "\n",
    "train_dataset = IMDBDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                       batch_size=BATCH_SIZE,\n",
    "                                       collate_fn=IMDB_collate_func,\n",
    "                                       shuffle=True)\n",
    "\n",
    "val_dataset = IMDBDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = IMDBDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=False)\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "lambda2 = lambda epoch:0.01-0.005*epoch\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda2)\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 5 # number epoch to train\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for epoch in range(num_epochs):\n",
    "    scheduler.step()\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "print('\\n'*2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [101/400], Validation Acc: 1.1\n",
      "Epoch: [1/5], Step: [201/400], Validation Acc: 65.26\n",
      "Epoch: [1/5], Step: [301/400], Validation Acc: 76.5\n",
      "Epoch: [2/5], Step: [101/400], Validation Acc: 76.9\n",
      "Epoch: [2/5], Step: [201/400], Validation Acc: 86.92\n",
      "Epoch: [2/5], Step: [301/400], Validation Acc: 73.44\n",
      "Epoch: [3/5], Step: [101/400], Validation Acc: 76.16\n",
      "Epoch: [3/5], Step: [201/400], Validation Acc: 75.16\n",
      "Epoch: [3/5], Step: [301/400], Validation Acc: 78.58\n",
      "Epoch: [4/5], Step: [101/400], Validation Acc: 82.04\n",
      "Epoch: [4/5], Step: [201/400], Validation Acc: 82.26\n",
      "Epoch: [4/5], Step: [301/400], Validation Acc: 85.74\n",
      "Epoch: [5/5], Step: [101/400], Validation Acc: 76.28\n",
      "Epoch: [5/5], Step: [201/400], Validation Acc: 77.3\n",
      "Epoch: [5/5], Step: [301/400], Validation Acc: 81.88\n",
      "After training for 5 epochs\n",
      "Test Acc 85.692\n"
     ]
    }
   ],
   "source": [
    "max_vocab_size = 30000\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "        token_counter = Counter(all_tokens)\n",
    "        vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "        id2token = list(vocab)\n",
    "        token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "        id2token = ['<pad>', '<unk>'] + id2token\n",
    "        token2id['<pad>'] = PAD_IDX \n",
    "        token2id['<unk>'] = UNK_IDX\n",
    "        return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens_2grams)\n",
    "\n",
    "# convert token to id in the dataset\n",
    "train_data_indices = token2index_dataset(train_data_tokens_2grams)\n",
    "val_data_indices = token2index_dataset(val_data_tokens_2grams)\n",
    "test_data_indices = token2index_dataset(test_data_tokens_2grams)\n",
    "\n",
    "train_dataset = IMDBDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                       batch_size=BATCH_SIZE,\n",
    "                                       collate_fn=IMDB_collate_func,\n",
    "                                       shuffle=True)\n",
    "\n",
    "val_dataset = IMDBDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = IMDBDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "learning_rate = 0.005\n",
    "num_epochs = 5 # number epoch to train\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "train_curve = []\n",
    "val_curve = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            val_curve.append(val_acc)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "            \n",
    "            train_acc = test_model(train_loader,model)\n",
    "            train_curve.append(train_acc)\n",
    "            \n",
    "            \n",
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1c5ad2bac8>"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_curve,label = \"Train set\")\n",
    "plt.plot(val_curve,label= \"Validation set\")\n",
    "plt.title('Training Curve')\n",
    "plt.xlabel('Every 100 Iterations')\n",
    "plt.ylabel('Prediction Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_clf,incor_clf = cor_incor_clf(val_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:\n",
      " ['I would have liked to give this movie a zero but that wasn\\'t an option!! This movie sucks!!! The women cannot act. i should have known it was gonna suck when i saw Bobby Brown. Nobody in my house could believe i hadn\\'t changed the channel after the first 15 minutes. the idea of black females as gunslingers in the western days is ridiculous. it\\'s not just a race thing, it\\'s also a gender. the combination of the two things is ridiculous.i am sorry because some of the people in the movie aren\\'t bad actors/actresses but the movie itself was awful. it was not credible as a movie. it might be \\'entertaining\\' to a certain group of people but i am not in that group. lol. and using a great line from a great, great movie...\"that\\'s all I have to say about that.\"', \"It's a waist to indulge such great actors in such a weak and boring movie. Besides all the unanswered questions posted in the other comments, what's so difficult about capturing the robbers? Just eliminate the bank workers, see who was at the bank-from all the cameras' footage angles-prior to the robbers entry and you have those extra 4 remaining robbers among the hostages. Where is the suspense every body is talking about? It was so obvious the moment the hostages were asked to change into this identical uniform that they were all going to walk out the front door... seen it many times. At least Mr. Spike Lee could have seasoned the movie with some good music score and artistic shooting. The Movie is not worth it. Pronto!\", 'This movie was so bad, outdated and stupid that I had rough times to watch it to the end. I had seen this Rodney guy in Natural Born Killers and I thought he was funny as hell in it, but this movie was crap. The \"jokes\" weren\\'t funny, actors weren\\'t funny, anything about it wasn\\'t even remotely funny. Don\\'t waste your time for this! Only positive things about this were the beautiful wives :) and Molly Shannon who I\\'m sure tried her best, but the script was just too awful. That\\'s why I rated it \"2\" instead of \"1\", but it\\'s definitely one of the worst films I\\'ve ever seen.']\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "Incorrect:\n",
      " ['Unusual? Yes! Unusual setting for an American wartime movie, New Zealand. Unusual subject matter, four sisters and their relationships with American soldiers, from one bearing the illegitimate child of the dead son of a Senator, to another living with seven Marines (one at a time) before being murdered by her returning POW New Zealander husband. Unusual to see Paul Newman deliver such a poor performance so soon after his unforgettable role as Rocky Graziano in the brilliant \"Somebody Up There Loves Me\". Unusual for two fine \"Stars\" Joan Fontaine and Jean Simmons, to leave so little of themselves on a movie. Unusual that I could be bothered to write a review of such a poor film, give it a miss!', \"I am probably one of the few viewers who would not recommend this film. Thought visually stunning like all of Ang Lee's work (each still frame seems worthy of a print), I was really disappointed by the film's disjointed pace. It really was too long. The story is set in Civil War era Missouri, and is about a young man (Roedel) who joins the feral forces of the Bushwackers, sort of renegade Confederate sympathizers who conduct geurilla type fighting with the Jayhawkers, their Union counterparts. He and his close friend, Jack Bull Chiles played by Skeet Ulrich, join the group after Chile's father is shot point-blank and his home is burned, presumably by Jayhawkers. The story follows Roedel's and Chiles' raiding adventures and their interactions with other victims of the war, including former slave who fights for the Bushwhackers (Daniel Holt played by Jeffery Wright), and a war widow played by Jewel. It seemed that every time the film developed the story to an interesting point, it would turn to some other subplot and leave things undeveloped. For example, the agitation among Roedel's group caused by former slave Holt participating in the confederate cause is shown briefly through some conflict regarding propriety and protocol, and then dropped until later in the movie. A young villian/bully Bushwhacker hates Roedel and directs much angst and violence against him, but, we never know why. Some of the characters never seem to surface; I think that is because the movie embraces too many of them as well as taking on large amounts of history. The historical detail was excellent. I loved looking at the housing, furniture, clothes, etc., and I thought the lead actors did a wonderful job of humanizing the characters, though they stumbled a bit with the dialog. Unless you really enjoy history or are a huge Ang Lee fan, though, take a pass on this one.\", \"Movies have put me to sleep before, but no movie has ever done that twice, so it took me three sittings actually to finish it. The dialog was bad. Women spoke stiltedly and the men were caricatures. And two of the supposedly Japanese women looked Chinese, had Chinese names and spoke with clearly Chinese accents. I'm still trying to figure out why the Emmenthal men were sexually wrapped up with each other. 10 minus 8 1/2 equals a tough choice: Do I give this movie a rating of one? or two? Movies have put me to sleep before, but no movie has ever done that twice, so it took me three sittings actually to finish it. The dialog was bad. Women spoke stiltedly and the men were caricatures. And two of the supposedly Japanese women looked Chinese, had Chinese names and spoke with clearly Chinese accents. I'm still trying to figure out why the Emmenthal men were sexually wrapped up with each other. 10 minus 8 1/2 equals a tough choice: Do I give this movie a rating of one? or two?\"]\n"
     ]
    }
   ],
   "source": [
    "print('Correct:\\n',cor_clf[:3])\n",
    "print('--'*58)\n",
    "print('Incorrect:\\n',incor_clf[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlpclass]",
   "language": "python",
   "name": "conda-env-nlpclass-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
